{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b16054cc-c1ef-46e1-ae1d-f0ff4c54a888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 [유관순] 문서 5개 검색 중...\n",
      "🔍 [청동기] 문서 5개 검색 중...\n",
      "🔍 [비파형 동검] 문서 5개 검색 중...\n",
      "✅ 저장 완료: 키워드별_5개문서_백과결과.xlsx\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import collections.abc\n",
    "collections.Callable = collections.abc.Callable\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# ✅ 셀레니움 드라이버 생성\n",
    "def create_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "# ✅ 키워드로 상위 N개 문서의 제목+URL 수집\n",
    "def get_article_urls_from_keyword(keyword, max_items=5):\n",
    "    driver = create_driver()\n",
    "    base_url = \"https://encykorea.aks.ac.kr\"\n",
    "    driver.get(base_url)\n",
    "    time.sleep(1)\n",
    "\n",
    "    search_input = driver.find_element(By.ID, \"keyword\")\n",
    "    search_input.clear()\n",
    "    search_input.send_keys(keyword)\n",
    "    driver.find_element(By.CLASS_NAME, \"main-search\").click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    items = driver.find_elements(By.CSS_SELECTOR, \"ul.encyclopedia-list li.item\")\n",
    "    results = []\n",
    "    for item in items[:max_items]:\n",
    "        try:\n",
    "            link_tag = item.find_element(By.CSS_SELECTOR, \"a\")\n",
    "            title_tag = item.find_element(By.CSS_SELECTOR, \".title\")\n",
    "            href = link_tag.get_attribute(\"href\")\n",
    "            title = title_tag.text.strip()\n",
    "            results.append((title, href))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    driver.quit()\n",
    "    return results\n",
    "\n",
    "# ✅ 본문 파싱\n",
    "def parse_article_contents(title, url, keyword):\n",
    "    try:\n",
    "        res = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        content = soup.find(\"div\", class_=\"contents-detail-contents\")\n",
    "        if not content:\n",
    "            return {\"키워드\": keyword, \"제목\": title, \"URL\": url, \"정의\": \"본문 없음\"}\n",
    "\n",
    "        sections = content.find_all(\"div\", class_=\"detail-section\")\n",
    "        result = {\"키워드\": keyword, \"제목\": title, \"URL\": url}\n",
    "        for sec in sections:\n",
    "            title_tag = sec.find(class_=\"section-title\")\n",
    "            body_tag = sec.find(class_=\"section-body\")\n",
    "            if title_tag and body_tag:\n",
    "                key = title_tag.get_text(strip=True)\n",
    "                value = body_tag.get_text(separator=\"\\n\").strip()\n",
    "                result[key] = value\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return {\"키워드\": keyword, \"제목\": title, \"URL\": url, \"정의\": f\"❌ 오류: {e}\"}\n",
    "\n",
    "# ✅ 전체 크롤링\n",
    "def crawl_multiple_articles_per_keyword(keywords, max_items=5):\n",
    "    all_results = []\n",
    "    for kw in keywords:\n",
    "        print(f\"🔍 [{kw}] 문서 {max_items}개 검색 중...\")\n",
    "        articles = get_article_urls_from_keyword(kw, max_items=max_items)\n",
    "        if not articles:\n",
    "            all_results.append({\"키워드\": kw, \"제목\": \"❌ 문서 없음\", \"URL\": \"\", \"정의\": \"N/A\"})\n",
    "            continue\n",
    "        for title, url in articles:\n",
    "            data = parse_article_contents(title, url, kw)\n",
    "            all_results.append(data)\n",
    "            time.sleep(1)\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# ✅ 키워드 예시\n",
    "keywords = [\"유관순\", \"청동기\", \"비파형 동검\"]\n",
    "df = crawl_multiple_articles_per_keyword(keywords, max_items=5)\n",
    "\n",
    "# ✅ 엑셀 저장\n",
    "df.to_excel(\"키워드별_5개문서_백과결과.xlsx\", index=False)\n",
    "print(\"✅ 저장 완료: 키워드별_5개문서_백과결과.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3637ad1-ca81-4c5f-b30e-8d3e4a3ce87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 (0) 키워드 '최윤덕' 문서 검색 중...\n",
      "🔍 (0) 키워드 '김종서' 문서 검색 중...\n",
      "🔍 (1) 키워드 '무역소' 문서 검색 중...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 111\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# ✅ 실행\u001b[39;00m\n\u001b[0;32m    110\u001b[0m excel_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_4.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# 실제 파일명\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m df_final \u001b[38;5;241m=\u001b[39m crawl_articles_from_excel_rows(excel_path, max_keywords\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, articles_per_keyword\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# ✅ 저장\u001b[39;00m\n\u001b[0;32m    114\u001b[0m df_final\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4번.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[10], line 92\u001b[0m, in \u001b[0;36mcrawl_articles_from_excel_rows\u001b[1;34m(excel_path, max_keywords, articles_per_keyword)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔍 (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) 키워드 \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkw\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m 문서 검색 중...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 92\u001b[0m articles \u001b[38;5;241m=\u001b[39m get_article_urls_from_keyword(kw, articles_per_keyword)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m articles:\n\u001b[0;32m     94\u001b[0m     result \u001b[38;5;241m=\u001b[39m row_info\u001b[38;5;241m.\u001b[39mcopy()\n",
      "Cell \u001b[1;32mIn[10], line 26\u001b[0m, in \u001b[0;36mget_article_urls_from_keyword\u001b[1;34m(keyword, max_items)\u001b[0m\n\u001b[0;32m     24\u001b[0m base_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://encykorea.aks.ac.kr\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     25\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(base_url)\n\u001b[1;32m---> 26\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     28\u001b[0m search_input \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mID, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeyword\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m search_input\u001b[38;5;241m.\u001b[39mclear()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import collections.abc\n",
    "collections.Callable = collections.abc.Callable\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# ✅ 셀레니움 드라이버 생성\n",
    "def create_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "# ✅ 키워드로 상위 N개 문서 검색\n",
    "def get_article_urls_from_keyword(keyword, max_items=5):\n",
    "    driver = create_driver()\n",
    "    base_url = \"https://encykorea.aks.ac.kr\"\n",
    "    driver.get(base_url)\n",
    "    time.sleep(1)\n",
    "\n",
    "    search_input = driver.find_element(By.ID, \"keyword\")\n",
    "    search_input.clear()\n",
    "    search_input.send_keys(keyword)\n",
    "    driver.find_element(By.CLASS_NAME, \"main-search\").click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    items = driver.find_elements(By.CSS_SELECTOR, \"ul.encyclopedia-list li.item\")\n",
    "    results = []\n",
    "    for item in items[:max_items]:\n",
    "        try:\n",
    "            link_tag = item.find_element(By.CSS_SELECTOR, \"a\")\n",
    "            title_tag = item.find_element(By.CSS_SELECTOR, \".title\")\n",
    "            href = link_tag.get_attribute(\"href\")\n",
    "            title = title_tag.text.strip()\n",
    "            results.append((title, href))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    driver.quit()\n",
    "    return results\n",
    "\n",
    "# ✅ 본문 파싱\n",
    "def parse_article_contents(title, url):\n",
    "    try:\n",
    "        res = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        content = soup.find(\"div\", class_=\"contents-detail-contents\")\n",
    "        if not content:\n",
    "            return {\"제목\": title, \"URL\": url, \"정의\": \"본문 없음\"}\n",
    "\n",
    "        sections = content.find_all(\"div\", class_=\"detail-section\")\n",
    "        result = {\"제목\": title, \"URL\": url}\n",
    "        for sec in sections:\n",
    "            title_tag = sec.find(class_=\"section-title\")\n",
    "            body_tag = sec.find(class_=\"section-body\")\n",
    "            if title_tag and body_tag:\n",
    "                key = title_tag.get_text(strip=True)\n",
    "                value = body_tag.get_text(separator=\"\\n\").strip()\n",
    "                result[key] = value\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return {\"제목\": title, \"URL\": url, \"정의\": f\"❌ 오류: {e}\"}\n",
    "\n",
    "# ✅ 원본 데이터 기반 크롤링 수행\n",
    "def crawl_articles_from_excel_rows(excel_path, max_keywords=5, articles_per_keyword=5):\n",
    "    df = pd.read_excel(excel_path)\n",
    "    all_results = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        row_info = {\n",
    "            \"열1\": row.get(\"열1\", \"\"),\n",
    "            \"열2\": row.get(\"열2\", \"\"),\n",
    "            \"열3\": row.get(\"열3\", \"\"),\n",
    "            \"열4\": row.get(\"열4\", \"\")\n",
    "        }\n",
    "\n",
    "        keywords = str(row_info[\"열4\"]).split(',') if pd.notna(row_info[\"열4\"]) else []\n",
    "        for kw in keywords[:max_keywords]:\n",
    "            kw = kw.strip()\n",
    "            if not kw:\n",
    "                continue\n",
    "\n",
    "            print(f\"🔍 ({idx}) 키워드 '{kw}' 문서 검색 중...\")\n",
    "            articles = get_article_urls_from_keyword(kw, articles_per_keyword)\n",
    "            if not articles:\n",
    "                result = row_info.copy()\n",
    "                result.update({\"키워드\": kw, \"제목\": \"❌ 문서 없음\", \"URL\": \"\", \"정의\": \"N/A\"})\n",
    "                all_results.append(result)\n",
    "                continue\n",
    "\n",
    "            for title, url in articles:\n",
    "                parsed = parse_article_contents(title, url)\n",
    "                result = row_info.copy()\n",
    "                result[\"키워드\"] = kw\n",
    "                result.update(parsed)\n",
    "                all_results.append(result)\n",
    "                time.sleep(1)\n",
    "\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# ✅ 실행\n",
    "excel_path = \"split_4.xlsx\"  # 실제 파일명\n",
    "df_final = crawl_articles_from_excel_rows(excel_path, max_keywords=2, articles_per_keyword=5)\n",
    "\n",
    "# ✅ 저장\n",
    "df_final.to_excel(\"4번.xlsx\", index=False)\n",
    "print(\"✅ 저장 완료: 교과서_키워드별_문서결과_5개씩.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b2b5551-9da8-4bbb-9481-f6ce75ec63ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖼️ (1) 키워드 '청동기' 이미지 검색 중...\n",
      "🖼️ (1) 키워드 '청동기시대' 이미지 검색 중...\n",
      "🖼️ (2) 키워드 '청동기 유물' 이미지 검색 중...\n",
      "✅ 저장 완료: 교과서_키워드_이미지결과.xlsx\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# ✅ 셀레니움 드라이버 생성 함수\n",
    "def create_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # 창 없이 실행\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "# ✅ 키워드로 미디어 탭 이미지 수집\n",
    "def get_media_results(keyword, max_items=5):\n",
    "    base_url = f\"https://encykorea.aks.ac.kr/Media/Search/{keyword}\"\n",
    "    driver = create_driver()\n",
    "    driver.get(base_url)\n",
    "    time.sleep(2)\n",
    "\n",
    "    items = driver.find_elements(By.CSS_SELECTOR, \"ul.media-list li.item\")\n",
    "    results = []\n",
    "\n",
    "    for item in items[:max_items]:\n",
    "        try:\n",
    "            title = item.find_element(By.CLASS_NAME, \"title\").text.strip()\n",
    "            img_tag = item.find_element(By.CSS_SELECTOR, \".media-img\")\n",
    "            style_attr = img_tag.get_attribute(\"style\")\n",
    "            start = style_attr.find('url(\"') + 5\n",
    "            end = style_attr.find('\")', start)\n",
    "            img_url = style_attr[start:end]\n",
    "            results.append({\"제목\": title, \"이미지 URL\": img_url})\n",
    "        except Exception as e:\n",
    "            print(\"❌ 항목 처리 실패:\", e)\n",
    "\n",
    "    driver.quit()\n",
    "    return results\n",
    "\n",
    "# ✅ 엑셀 기반으로 이미지 크롤링 통합\n",
    "def crawl_media_from_excel(excel_path, max_keywords=3, images_per_keyword=5):\n",
    "    df = pd.read_excel(excel_path)[:3]\n",
    "    all_results = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        row_info = {\n",
    "            \"열1\": row.get(\"열1\", \"\"),\n",
    "            \"열2\": row.get(\"열2\", \"\"),\n",
    "            \"열3\": row.get(\"열3\", \"\"),\n",
    "            \"열4\": row.get(\"열4\", \"\")\n",
    "        }\n",
    "\n",
    "        keywords = str(row_info[\"열4\"]).split(',') if pd.notna(row_info[\"열4\"]) else []\n",
    "        for kw in keywords[:max_keywords]:\n",
    "            kw = kw.strip()\n",
    "            if not kw:\n",
    "                continue\n",
    "\n",
    "            print(f\"🖼️ ({idx}) 키워드 '{kw}' 이미지 검색 중...\")\n",
    "            images = get_media_results(kw, max_items=images_per_keyword)\n",
    "            if not images:\n",
    "                result = row_info.copy()\n",
    "                result.update({\"키워드\": kw, \"제목\": \"❌ 이미지 없음\", \"이미지 URL\": \"\"})\n",
    "                all_results.append(result)\n",
    "                continue\n",
    "\n",
    "            for img in images:\n",
    "                result = row_info.copy()\n",
    "                result[\"키워드\"] = kw\n",
    "                result.update(img)\n",
    "                all_results.append(result)\n",
    "                time.sleep(0.5)\n",
    "\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# ✅ 실행\n",
    "if __name__ == \"__main__\":\n",
    "    excel_path = \"교과서_본문_정리_열이름통일 (3).xlsx\"  # 실제 파일명\n",
    "    df_media_final = crawl_media_from_excel(excel_path, max_keywords=2, images_per_keyword=5)\n",
    "    df_media_final.to_excel(\"교과서_키워드_이미지결과.xlsx\", index=False)\n",
    "    print(\"✅ 저장 완료: 교과서_키워드_이미지결과.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b06954cc-36ee-43b0-8d74-cda75f410b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 (50) 키워드 '도요토미 히데요시' 문서 검색 중...\n",
      "🔍 (51) 키워드 '임진왜란' 문서 검색 중...\n",
      "🔍 (51) 키워드 '부산성' 문서 검색 중...\n",
      "🔍 (52) 키워드 '임진왜란' 문서 검색 중...\n",
      "🔍 (52) 키워드 '수군' 문서 검색 중...\n",
      "🔍 (53) 키워드 '이순신' 문서 검색 중...\n",
      "🔍 (53) 키워드 '옥포해전' 문서 검색 중...\n",
      "🔍 (54) 키워드 '의병' 문서 검색 중...\n",
      "🔍 (54) 키워드 '승군' 문서 검색 중...\n",
      "🔍 (56) 키워드 '정유재란' 문서 검색 중...\n",
      "✅ 저장 완료: 4번_1-1.xlsx\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 118\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ 저장 완료: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# ✅ 실행\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m crawl_articles_from_excel_rows(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_4.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_keywords\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, articles_per_keyword\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 93\u001b[0m, in \u001b[0;36mcrawl_articles_from_excel_rows\u001b[1;34m(excel_path, max_keywords, articles_per_keyword, save_path)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔍 (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) 키워드 \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkw\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m 문서 검색 중...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m articles \u001b[38;5;241m=\u001b[39m get_article_urls_from_keyword(kw, articles_per_keyword)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m articles:\n\u001b[0;32m     95\u001b[0m     result \u001b[38;5;241m=\u001b[39m row_info\u001b[38;5;241m.\u001b[39mcopy()\n",
      "Cell \u001b[1;32mIn[1], line 25\u001b[0m, in \u001b[0;36mget_article_urls_from_keyword\u001b[1;34m(keyword, max_items)\u001b[0m\n\u001b[0;32m     23\u001b[0m driver \u001b[38;5;241m=\u001b[39m create_driver()\n\u001b[0;32m     24\u001b[0m base_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://encykorea.aks.ac.kr\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 25\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(base_url)\n\u001b[0;32m     26\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     28\u001b[0m search_input \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mID, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeyword\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:472\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    455\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Navigate the browser to the specified URL in the current window or\u001b[39;00m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;124;03m    tab.\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;124;03m    >>> driver.get(\"https://example.com\")\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mGET, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: url})\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:445\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m    443\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[1;32m--> 445\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:404\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    402\u001b[0m trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trim_large_entries(params)\n\u001b[0;32m    403\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[1;32m--> 404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(command_info[\u001b[38;5;241m0\u001b[39m], url, body\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:428\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    425\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_config\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[1;32m--> 428\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn\u001b[38;5;241m.\u001b[39mrequest(method, url, body\u001b[38;5;241m=\u001b[39mbody, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_config\u001b[38;5;241m.\u001b[39mtimeout)\n\u001b[0;32m    429\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\_request_methods.py:143\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[1;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[0;32m    136\u001b[0m         method,\n\u001b[0;32m    137\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw,\n\u001b[0;32m    141\u001b[0m     )\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_body(\n\u001b[0;32m    144\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[0;32m    145\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\_request_methods.py:278\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m, content_type)\n\u001b[0;32m    276\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[1;32m--> 278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\poolmanager.py:443\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    441\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 443\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, u\u001b[38;5;241m.\u001b[39mrequest_uri, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    445\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1428\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import collections.abc\n",
    "collections.Callable = collections.abc.Callable\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# ✅ 셀레니움 드라이버 생성\n",
    "def create_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "# ✅ 키워드로 상위 N개 문서 검색\n",
    "def get_article_urls_from_keyword(keyword, max_items=5):\n",
    "    driver = create_driver()\n",
    "    base_url = \"https://encykorea.aks.ac.kr\"\n",
    "    driver.get(base_url)\n",
    "    time.sleep(1)\n",
    "\n",
    "    search_input = driver.find_element(By.ID, \"keyword\")\n",
    "    search_input.clear()\n",
    "    search_input.send_keys(keyword)\n",
    "    driver.find_element(By.CLASS_NAME, \"main-search\").click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    items = driver.find_elements(By.CSS_SELECTOR, \"ul.encyclopedia-list li.item\")\n",
    "    results = []\n",
    "    for item in items[:max_items]:\n",
    "        try:\n",
    "            link_tag = item.find_element(By.CSS_SELECTOR, \"a\")\n",
    "            title_tag = item.find_element(By.CSS_SELECTOR, \".title\")\n",
    "            href = link_tag.get_attribute(\"href\")\n",
    "            title = title_tag.text.strip()\n",
    "            results.append((title, href))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    driver.quit()\n",
    "    return results\n",
    "\n",
    "# ✅ 본문 파싱\n",
    "def parse_article_contents(title, url):\n",
    "    try:\n",
    "        res = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        content = soup.find(\"div\", class_=\"contents-detail-contents\")\n",
    "        if not content:\n",
    "            return {\"제목\": title, \"URL\": url, \"정의\": \"본문 없음\"}\n",
    "\n",
    "        sections = content.find_all(\"div\", class_=\"detail-section\")\n",
    "        result = {\"제목\": title, \"URL\": url}\n",
    "        for sec in sections:\n",
    "            title_tag = sec.find(class_=\"section-title\")\n",
    "            body_tag = sec.find(class_=\"section-body\")\n",
    "            if title_tag and body_tag:\n",
    "                key = title_tag.get_text(strip=True)\n",
    "                value = body_tag.get_text(separator=\"\\n\").strip()\n",
    "                result[key] = value\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return {\"제목\": title, \"URL\": url, \"정의\": f\"❌ 오류: {e}\"}\n",
    "\n",
    "# ✅ 원본 데이터 기반 크롤링 수행\n",
    "def crawl_articles_from_excel_rows(excel_path, max_keywords=5, articles_per_keyword=5, save_path=\"4번_1-1.xlsx\"):\n",
    "    df = pd.read_excel(excel_path)[50:]\n",
    "    all_results = []\n",
    "\n",
    "    try:\n",
    "        for idx, row in df.iterrows():\n",
    "            row_info = {\n",
    "                \"열1\": row.get(\"열1\", \"\"),\n",
    "                \"열2\": row.get(\"열2\", \"\"),\n",
    "                \"열3\": row.get(\"열3\", \"\"),\n",
    "                \"열4\": row.get(\"열4\", \"\")\n",
    "            }\n",
    "\n",
    "            keywords = str(row_info[\"열4\"]).split(',') if pd.notna(row_info[\"열4\"]) else []\n",
    "            for kw in keywords[:max_keywords]:\n",
    "                kw = kw.strip()\n",
    "                if not kw:\n",
    "                    continue\n",
    "\n",
    "                print(f\"🔍 ({idx}) 키워드 '{kw}' 문서 검색 중...\")\n",
    "                articles = get_article_urls_from_keyword(kw, articles_per_keyword)\n",
    "                if not articles:\n",
    "                    result = row_info.copy()\n",
    "                    result.update({\"키워드\": kw, \"제목\": \"❌ 문서 없음\", \"URL\": \"\", \"정의\": \"N/A\"})\n",
    "                    all_results.append(result)\n",
    "                    continue\n",
    "\n",
    "                for title, url in articles:\n",
    "                    parsed = parse_article_contents(title, url)\n",
    "                    result = row_info.copy()\n",
    "                    result[\"키워드\"] = kw\n",
    "                    result.update(parsed)\n",
    "                    all_results.append(result)\n",
    "                    time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 오류 발생: {e}\")\n",
    "        print(\"⚠️ 수집된 데이터까지 저장 후 종료합니다...\")\n",
    "\n",
    "    finally:\n",
    "        final_df = pd.DataFrame(all_results)\n",
    "        final_df.to_excel(save_path, index=False)\n",
    "        print(f\"✅ 저장 완료: {save_path}\")\n",
    "\n",
    "# ✅ 실행\n",
    "crawl_articles_from_excel_rows(\"split_4.xlsx\", max_keywords=2, articles_per_keyword=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12ff72d-9f8c-4b27-bef0-4ea0008a8865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
