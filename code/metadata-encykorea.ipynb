{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b16054cc-c1ef-46e1-ae1d-f0ff4c54a888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” [ìœ ê´€ìˆœ] ë¬¸ì„œ 5ê°œ ê²€ìƒ‰ ì¤‘...\n",
      "ğŸ” [ì²­ë™ê¸°] ë¬¸ì„œ 5ê°œ ê²€ìƒ‰ ì¤‘...\n",
      "ğŸ” [ë¹„íŒŒí˜• ë™ê²€] ë¬¸ì„œ 5ê°œ ê²€ìƒ‰ ì¤‘...\n",
      "âœ… ì €ì¥ ì™„ë£Œ: í‚¤ì›Œë“œë³„_5ê°œë¬¸ì„œ_ë°±ê³¼ê²°ê³¼.xlsx\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import collections.abc\n",
    "collections.Callable = collections.abc.Callable\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# âœ… ì…€ë ˆë‹ˆì›€ ë“œë¼ì´ë²„ ìƒì„±\n",
    "def create_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "# âœ… í‚¤ì›Œë“œë¡œ ìƒìœ„ Nê°œ ë¬¸ì„œì˜ ì œëª©+URL ìˆ˜ì§‘\n",
    "def get_article_urls_from_keyword(keyword, max_items=5):\n",
    "    driver = create_driver()\n",
    "    base_url = \"https://encykorea.aks.ac.kr\"\n",
    "    driver.get(base_url)\n",
    "    time.sleep(1)\n",
    "\n",
    "    search_input = driver.find_element(By.ID, \"keyword\")\n",
    "    search_input.clear()\n",
    "    search_input.send_keys(keyword)\n",
    "    driver.find_element(By.CLASS_NAME, \"main-search\").click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    items = driver.find_elements(By.CSS_SELECTOR, \"ul.encyclopedia-list li.item\")\n",
    "    results = []\n",
    "    for item in items[:max_items]:\n",
    "        try:\n",
    "            link_tag = item.find_element(By.CSS_SELECTOR, \"a\")\n",
    "            title_tag = item.find_element(By.CSS_SELECTOR, \".title\")\n",
    "            href = link_tag.get_attribute(\"href\")\n",
    "            title = title_tag.text.strip()\n",
    "            results.append((title, href))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    driver.quit()\n",
    "    return results\n",
    "\n",
    "# âœ… ë³¸ë¬¸ íŒŒì‹±\n",
    "def parse_article_contents(title, url, keyword):\n",
    "    try:\n",
    "        res = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        content = soup.find(\"div\", class_=\"contents-detail-contents\")\n",
    "        if not content:\n",
    "            return {\"í‚¤ì›Œë“œ\": keyword, \"ì œëª©\": title, \"URL\": url, \"ì •ì˜\": \"ë³¸ë¬¸ ì—†ìŒ\"}\n",
    "\n",
    "        sections = content.find_all(\"div\", class_=\"detail-section\")\n",
    "        result = {\"í‚¤ì›Œë“œ\": keyword, \"ì œëª©\": title, \"URL\": url}\n",
    "        for sec in sections:\n",
    "            title_tag = sec.find(class_=\"section-title\")\n",
    "            body_tag = sec.find(class_=\"section-body\")\n",
    "            if title_tag and body_tag:\n",
    "                key = title_tag.get_text(strip=True)\n",
    "                value = body_tag.get_text(separator=\"\\n\").strip()\n",
    "                result[key] = value\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return {\"í‚¤ì›Œë“œ\": keyword, \"ì œëª©\": title, \"URL\": url, \"ì •ì˜\": f\"âŒ ì˜¤ë¥˜: {e}\"}\n",
    "\n",
    "# âœ… ì „ì²´ í¬ë¡¤ë§\n",
    "def crawl_multiple_articles_per_keyword(keywords, max_items=5):\n",
    "    all_results = []\n",
    "    for kw in keywords:\n",
    "        print(f\"ğŸ” [{kw}] ë¬¸ì„œ {max_items}ê°œ ê²€ìƒ‰ ì¤‘...\")\n",
    "        articles = get_article_urls_from_keyword(kw, max_items=max_items)\n",
    "        if not articles:\n",
    "            all_results.append({\"í‚¤ì›Œë“œ\": kw, \"ì œëª©\": \"âŒ ë¬¸ì„œ ì—†ìŒ\", \"URL\": \"\", \"ì •ì˜\": \"N/A\"})\n",
    "            continue\n",
    "        for title, url in articles:\n",
    "            data = parse_article_contents(title, url, kw)\n",
    "            all_results.append(data)\n",
    "            time.sleep(1)\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# âœ… í‚¤ì›Œë“œ ì˜ˆì‹œ\n",
    "keywords = [\"ìœ ê´€ìˆœ\", \"ì²­ë™ê¸°\", \"ë¹„íŒŒí˜• ë™ê²€\"]\n",
    "df = crawl_multiple_articles_per_keyword(keywords, max_items=5)\n",
    "\n",
    "# âœ… ì—‘ì…€ ì €ì¥\n",
    "df.to_excel(\"í‚¤ì›Œë“œë³„_5ê°œë¬¸ì„œ_ë°±ê³¼ê²°ê³¼.xlsx\", index=False)\n",
    "print(\"âœ… ì €ì¥ ì™„ë£Œ: í‚¤ì›Œë“œë³„_5ê°œë¬¸ì„œ_ë°±ê³¼ê²°ê³¼.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3637ad1-ca81-4c5f-b30e-8d3e4a3ce87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” (0) í‚¤ì›Œë“œ 'ìµœìœ¤ë•' ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...\n",
      "ğŸ” (0) í‚¤ì›Œë“œ 'ê¹€ì¢…ì„œ' ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...\n",
      "ğŸ” (1) í‚¤ì›Œë“œ 'ë¬´ì—­ì†Œ' ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 111\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# âœ… ì‹¤í–‰\u001b[39;00m\n\u001b[0;32m    110\u001b[0m excel_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_4.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# ì‹¤ì œ íŒŒì¼ëª…\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m df_final \u001b[38;5;241m=\u001b[39m crawl_articles_from_excel_rows(excel_path, max_keywords\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, articles_per_keyword\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# âœ… ì €ì¥\u001b[39;00m\n\u001b[0;32m    114\u001b[0m df_final\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4ë²ˆ.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[10], line 92\u001b[0m, in \u001b[0;36mcrawl_articles_from_excel_rows\u001b[1;34m(excel_path, max_keywords, articles_per_keyword)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ” (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) í‚¤ì›Œë“œ \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkw\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 92\u001b[0m articles \u001b[38;5;241m=\u001b[39m get_article_urls_from_keyword(kw, articles_per_keyword)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m articles:\n\u001b[0;32m     94\u001b[0m     result \u001b[38;5;241m=\u001b[39m row_info\u001b[38;5;241m.\u001b[39mcopy()\n",
      "Cell \u001b[1;32mIn[10], line 26\u001b[0m, in \u001b[0;36mget_article_urls_from_keyword\u001b[1;34m(keyword, max_items)\u001b[0m\n\u001b[0;32m     24\u001b[0m base_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://encykorea.aks.ac.kr\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     25\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(base_url)\n\u001b[1;32m---> 26\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     28\u001b[0m search_input \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mID, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeyword\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m search_input\u001b[38;5;241m.\u001b[39mclear()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import collections.abc\n",
    "collections.Callable = collections.abc.Callable\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# âœ… ì…€ë ˆë‹ˆì›€ ë“œë¼ì´ë²„ ìƒì„±\n",
    "def create_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "# âœ… í‚¤ì›Œë“œë¡œ ìƒìœ„ Nê°œ ë¬¸ì„œ ê²€ìƒ‰\n",
    "def get_article_urls_from_keyword(keyword, max_items=5):\n",
    "    driver = create_driver()\n",
    "    base_url = \"https://encykorea.aks.ac.kr\"\n",
    "    driver.get(base_url)\n",
    "    time.sleep(1)\n",
    "\n",
    "    search_input = driver.find_element(By.ID, \"keyword\")\n",
    "    search_input.clear()\n",
    "    search_input.send_keys(keyword)\n",
    "    driver.find_element(By.CLASS_NAME, \"main-search\").click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    items = driver.find_elements(By.CSS_SELECTOR, \"ul.encyclopedia-list li.item\")\n",
    "    results = []\n",
    "    for item in items[:max_items]:\n",
    "        try:\n",
    "            link_tag = item.find_element(By.CSS_SELECTOR, \"a\")\n",
    "            title_tag = item.find_element(By.CSS_SELECTOR, \".title\")\n",
    "            href = link_tag.get_attribute(\"href\")\n",
    "            title = title_tag.text.strip()\n",
    "            results.append((title, href))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    driver.quit()\n",
    "    return results\n",
    "\n",
    "# âœ… ë³¸ë¬¸ íŒŒì‹±\n",
    "def parse_article_contents(title, url):\n",
    "    try:\n",
    "        res = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        content = soup.find(\"div\", class_=\"contents-detail-contents\")\n",
    "        if not content:\n",
    "            return {\"ì œëª©\": title, \"URL\": url, \"ì •ì˜\": \"ë³¸ë¬¸ ì—†ìŒ\"}\n",
    "\n",
    "        sections = content.find_all(\"div\", class_=\"detail-section\")\n",
    "        result = {\"ì œëª©\": title, \"URL\": url}\n",
    "        for sec in sections:\n",
    "            title_tag = sec.find(class_=\"section-title\")\n",
    "            body_tag = sec.find(class_=\"section-body\")\n",
    "            if title_tag and body_tag:\n",
    "                key = title_tag.get_text(strip=True)\n",
    "                value = body_tag.get_text(separator=\"\\n\").strip()\n",
    "                result[key] = value\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return {\"ì œëª©\": title, \"URL\": url, \"ì •ì˜\": f\"âŒ ì˜¤ë¥˜: {e}\"}\n",
    "\n",
    "# âœ… ì›ë³¸ ë°ì´í„° ê¸°ë°˜ í¬ë¡¤ë§ ìˆ˜í–‰\n",
    "def crawl_articles_from_excel_rows(excel_path, max_keywords=5, articles_per_keyword=5):\n",
    "    df = pd.read_excel(excel_path)\n",
    "    all_results = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        row_info = {\n",
    "            \"ì—´1\": row.get(\"ì—´1\", \"\"),\n",
    "            \"ì—´2\": row.get(\"ì—´2\", \"\"),\n",
    "            \"ì—´3\": row.get(\"ì—´3\", \"\"),\n",
    "            \"ì—´4\": row.get(\"ì—´4\", \"\")\n",
    "        }\n",
    "\n",
    "        keywords = str(row_info[\"ì—´4\"]).split(',') if pd.notna(row_info[\"ì—´4\"]) else []\n",
    "        for kw in keywords[:max_keywords]:\n",
    "            kw = kw.strip()\n",
    "            if not kw:\n",
    "                continue\n",
    "\n",
    "            print(f\"ğŸ” ({idx}) í‚¤ì›Œë“œ '{kw}' ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...\")\n",
    "            articles = get_article_urls_from_keyword(kw, articles_per_keyword)\n",
    "            if not articles:\n",
    "                result = row_info.copy()\n",
    "                result.update({\"í‚¤ì›Œë“œ\": kw, \"ì œëª©\": \"âŒ ë¬¸ì„œ ì—†ìŒ\", \"URL\": \"\", \"ì •ì˜\": \"N/A\"})\n",
    "                all_results.append(result)\n",
    "                continue\n",
    "\n",
    "            for title, url in articles:\n",
    "                parsed = parse_article_contents(title, url)\n",
    "                result = row_info.copy()\n",
    "                result[\"í‚¤ì›Œë“œ\"] = kw\n",
    "                result.update(parsed)\n",
    "                all_results.append(result)\n",
    "                time.sleep(1)\n",
    "\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# âœ… ì‹¤í–‰\n",
    "excel_path = \"split_4.xlsx\"  # ì‹¤ì œ íŒŒì¼ëª…\n",
    "df_final = crawl_articles_from_excel_rows(excel_path, max_keywords=2, articles_per_keyword=5)\n",
    "\n",
    "# âœ… ì €ì¥\n",
    "df_final.to_excel(\"4ë²ˆ.xlsx\", index=False)\n",
    "print(\"âœ… ì €ì¥ ì™„ë£Œ: êµê³¼ì„œ_í‚¤ì›Œë“œë³„_ë¬¸ì„œê²°ê³¼_5ê°œì”©.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b2b5551-9da8-4bbb-9481-f6ce75ec63ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¼ï¸ (1) í‚¤ì›Œë“œ 'ì²­ë™ê¸°' ì´ë¯¸ì§€ ê²€ìƒ‰ ì¤‘...\n",
      "ğŸ–¼ï¸ (1) í‚¤ì›Œë“œ 'ì²­ë™ê¸°ì‹œëŒ€' ì´ë¯¸ì§€ ê²€ìƒ‰ ì¤‘...\n",
      "ğŸ–¼ï¸ (2) í‚¤ì›Œë“œ 'ì²­ë™ê¸° ìœ ë¬¼' ì´ë¯¸ì§€ ê²€ìƒ‰ ì¤‘...\n",
      "âœ… ì €ì¥ ì™„ë£Œ: êµê³¼ì„œ_í‚¤ì›Œë“œ_ì´ë¯¸ì§€ê²°ê³¼.xlsx\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# âœ… ì…€ë ˆë‹ˆì›€ ë“œë¼ì´ë²„ ìƒì„± í•¨ìˆ˜\n",
    "def create_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # ì°½ ì—†ì´ ì‹¤í–‰\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "# âœ… í‚¤ì›Œë“œë¡œ ë¯¸ë””ì–´ íƒ­ ì´ë¯¸ì§€ ìˆ˜ì§‘\n",
    "def get_media_results(keyword, max_items=5):\n",
    "    base_url = f\"https://encykorea.aks.ac.kr/Media/Search/{keyword}\"\n",
    "    driver = create_driver()\n",
    "    driver.get(base_url)\n",
    "    time.sleep(2)\n",
    "\n",
    "    items = driver.find_elements(By.CSS_SELECTOR, \"ul.media-list li.item\")\n",
    "    results = []\n",
    "\n",
    "    for item in items[:max_items]:\n",
    "        try:\n",
    "            title = item.find_element(By.CLASS_NAME, \"title\").text.strip()\n",
    "            img_tag = item.find_element(By.CSS_SELECTOR, \".media-img\")\n",
    "            style_attr = img_tag.get_attribute(\"style\")\n",
    "            start = style_attr.find('url(\"') + 5\n",
    "            end = style_attr.find('\")', start)\n",
    "            img_url = style_attr[start:end]\n",
    "            results.append({\"ì œëª©\": title, \"ì´ë¯¸ì§€ URL\": img_url})\n",
    "        except Exception as e:\n",
    "            print(\"âŒ í•­ëª© ì²˜ë¦¬ ì‹¤íŒ¨:\", e)\n",
    "\n",
    "    driver.quit()\n",
    "    return results\n",
    "\n",
    "# âœ… ì—‘ì…€ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ í¬ë¡¤ë§ í†µí•©\n",
    "def crawl_media_from_excel(excel_path, max_keywords=3, images_per_keyword=5):\n",
    "    df = pd.read_excel(excel_path)[:3]\n",
    "    all_results = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        row_info = {\n",
    "            \"ì—´1\": row.get(\"ì—´1\", \"\"),\n",
    "            \"ì—´2\": row.get(\"ì—´2\", \"\"),\n",
    "            \"ì—´3\": row.get(\"ì—´3\", \"\"),\n",
    "            \"ì—´4\": row.get(\"ì—´4\", \"\")\n",
    "        }\n",
    "\n",
    "        keywords = str(row_info[\"ì—´4\"]).split(',') if pd.notna(row_info[\"ì—´4\"]) else []\n",
    "        for kw in keywords[:max_keywords]:\n",
    "            kw = kw.strip()\n",
    "            if not kw:\n",
    "                continue\n",
    "\n",
    "            print(f\"ğŸ–¼ï¸ ({idx}) í‚¤ì›Œë“œ '{kw}' ì´ë¯¸ì§€ ê²€ìƒ‰ ì¤‘...\")\n",
    "            images = get_media_results(kw, max_items=images_per_keyword)\n",
    "            if not images:\n",
    "                result = row_info.copy()\n",
    "                result.update({\"í‚¤ì›Œë“œ\": kw, \"ì œëª©\": \"âŒ ì´ë¯¸ì§€ ì—†ìŒ\", \"ì´ë¯¸ì§€ URL\": \"\"})\n",
    "                all_results.append(result)\n",
    "                continue\n",
    "\n",
    "            for img in images:\n",
    "                result = row_info.copy()\n",
    "                result[\"í‚¤ì›Œë“œ\"] = kw\n",
    "                result.update(img)\n",
    "                all_results.append(result)\n",
    "                time.sleep(0.5)\n",
    "\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# âœ… ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    excel_path = \"êµê³¼ì„œ_ë³¸ë¬¸_ì •ë¦¬_ì—´ì´ë¦„í†µì¼ (3).xlsx\"  # ì‹¤ì œ íŒŒì¼ëª…\n",
    "    df_media_final = crawl_media_from_excel(excel_path, max_keywords=2, images_per_keyword=5)\n",
    "    df_media_final.to_excel(\"êµê³¼ì„œ_í‚¤ì›Œë“œ_ì´ë¯¸ì§€ê²°ê³¼.xlsx\", index=False)\n",
    "    print(\"âœ… ì €ì¥ ì™„ë£Œ: êµê³¼ì„œ_í‚¤ì›Œë“œ_ì´ë¯¸ì§€ê²°ê³¼.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b06954cc-36ee-43b0-8d74-cda75f410b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” (50) í‚¤ì›Œë“œ 'ë„ìš”í† ë¯¸ íˆë°ìš”ì‹œ' ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...\n",
      "ğŸ” (51) í‚¤ì›Œë“œ 'ì„ì§„ì™œë€' ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...\n",
      "ğŸ” (51) í‚¤ì›Œë“œ 'ë¶€ì‚°ì„±' ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...\n",
      "ğŸ” (52) í‚¤ì›Œë“œ 'ì„ì§„ì™œë€' ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...\n",
      "ğŸ” (52) í‚¤ì›Œë“œ 'ìˆ˜êµ°' ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...\n",
      "ğŸ” (53) í‚¤ì›Œë“œ 'ì´ìˆœì‹ ' ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...\n",
      "ğŸ” (53) í‚¤ì›Œë“œ 'ì˜¥í¬í•´ì „' ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...\n",
      "ğŸ” (54) í‚¤ì›Œë“œ 'ì˜ë³‘' ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...\n",
      "ğŸ” (54) í‚¤ì›Œë“œ 'ìŠ¹êµ°' ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...\n",
      "ğŸ” (56) í‚¤ì›Œë“œ 'ì •ìœ ì¬ë€' ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...\n",
      "âœ… ì €ì¥ ì™„ë£Œ: 4ë²ˆ_1-1.xlsx\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 118\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… ì €ì¥ ì™„ë£Œ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# âœ… ì‹¤í–‰\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m crawl_articles_from_excel_rows(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_4.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_keywords\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, articles_per_keyword\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 93\u001b[0m, in \u001b[0;36mcrawl_articles_from_excel_rows\u001b[1;34m(excel_path, max_keywords, articles_per_keyword, save_path)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ” (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) í‚¤ì›Œë“œ \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkw\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m articles \u001b[38;5;241m=\u001b[39m get_article_urls_from_keyword(kw, articles_per_keyword)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m articles:\n\u001b[0;32m     95\u001b[0m     result \u001b[38;5;241m=\u001b[39m row_info\u001b[38;5;241m.\u001b[39mcopy()\n",
      "Cell \u001b[1;32mIn[1], line 25\u001b[0m, in \u001b[0;36mget_article_urls_from_keyword\u001b[1;34m(keyword, max_items)\u001b[0m\n\u001b[0;32m     23\u001b[0m driver \u001b[38;5;241m=\u001b[39m create_driver()\n\u001b[0;32m     24\u001b[0m base_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://encykorea.aks.ac.kr\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 25\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(base_url)\n\u001b[0;32m     26\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     28\u001b[0m search_input \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mID, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeyword\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:472\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    455\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Navigate the browser to the specified URL in the current window or\u001b[39;00m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;124;03m    tab.\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;124;03m    >>> driver.get(\"https://example.com\")\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mGET, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: url})\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:445\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m    443\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[1;32m--> 445\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:404\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    402\u001b[0m trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trim_large_entries(params)\n\u001b[0;32m    403\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[1;32m--> 404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(command_info[\u001b[38;5;241m0\u001b[39m], url, body\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:428\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    425\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_config\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[1;32m--> 428\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn\u001b[38;5;241m.\u001b[39mrequest(method, url, body\u001b[38;5;241m=\u001b[39mbody, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_config\u001b[38;5;241m.\u001b[39mtimeout)\n\u001b[0;32m    429\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\_request_methods.py:143\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[1;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[0;32m    136\u001b[0m         method,\n\u001b[0;32m    137\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw,\n\u001b[0;32m    141\u001b[0m     )\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_body(\n\u001b[0;32m    144\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[0;32m    145\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\_request_methods.py:278\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m, content_type)\n\u001b[0;32m    276\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[1;32m--> 278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\poolmanager.py:443\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    441\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 443\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, u\u001b[38;5;241m.\u001b[39mrequest_uri, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    445\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1428\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import collections.abc\n",
    "collections.Callable = collections.abc.Callable\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# âœ… ì…€ë ˆë‹ˆì›€ ë“œë¼ì´ë²„ ìƒì„±\n",
    "def create_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "# âœ… í‚¤ì›Œë“œë¡œ ìƒìœ„ Nê°œ ë¬¸ì„œ ê²€ìƒ‰\n",
    "def get_article_urls_from_keyword(keyword, max_items=5):\n",
    "    driver = create_driver()\n",
    "    base_url = \"https://encykorea.aks.ac.kr\"\n",
    "    driver.get(base_url)\n",
    "    time.sleep(1)\n",
    "\n",
    "    search_input = driver.find_element(By.ID, \"keyword\")\n",
    "    search_input.clear()\n",
    "    search_input.send_keys(keyword)\n",
    "    driver.find_element(By.CLASS_NAME, \"main-search\").click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    items = driver.find_elements(By.CSS_SELECTOR, \"ul.encyclopedia-list li.item\")\n",
    "    results = []\n",
    "    for item in items[:max_items]:\n",
    "        try:\n",
    "            link_tag = item.find_element(By.CSS_SELECTOR, \"a\")\n",
    "            title_tag = item.find_element(By.CSS_SELECTOR, \".title\")\n",
    "            href = link_tag.get_attribute(\"href\")\n",
    "            title = title_tag.text.strip()\n",
    "            results.append((title, href))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    driver.quit()\n",
    "    return results\n",
    "\n",
    "# âœ… ë³¸ë¬¸ íŒŒì‹±\n",
    "def parse_article_contents(title, url):\n",
    "    try:\n",
    "        res = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        content = soup.find(\"div\", class_=\"contents-detail-contents\")\n",
    "        if not content:\n",
    "            return {\"ì œëª©\": title, \"URL\": url, \"ì •ì˜\": \"ë³¸ë¬¸ ì—†ìŒ\"}\n",
    "\n",
    "        sections = content.find_all(\"div\", class_=\"detail-section\")\n",
    "        result = {\"ì œëª©\": title, \"URL\": url}\n",
    "        for sec in sections:\n",
    "            title_tag = sec.find(class_=\"section-title\")\n",
    "            body_tag = sec.find(class_=\"section-body\")\n",
    "            if title_tag and body_tag:\n",
    "                key = title_tag.get_text(strip=True)\n",
    "                value = body_tag.get_text(separator=\"\\n\").strip()\n",
    "                result[key] = value\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return {\"ì œëª©\": title, \"URL\": url, \"ì •ì˜\": f\"âŒ ì˜¤ë¥˜: {e}\"}\n",
    "\n",
    "# âœ… ì›ë³¸ ë°ì´í„° ê¸°ë°˜ í¬ë¡¤ë§ ìˆ˜í–‰\n",
    "def crawl_articles_from_excel_rows(excel_path, max_keywords=5, articles_per_keyword=5, save_path=\"4ë²ˆ_1-1.xlsx\"):\n",
    "    df = pd.read_excel(excel_path)[50:]\n",
    "    all_results = []\n",
    "\n",
    "    try:\n",
    "        for idx, row in df.iterrows():\n",
    "            row_info = {\n",
    "                \"ì—´1\": row.get(\"ì—´1\", \"\"),\n",
    "                \"ì—´2\": row.get(\"ì—´2\", \"\"),\n",
    "                \"ì—´3\": row.get(\"ì—´3\", \"\"),\n",
    "                \"ì—´4\": row.get(\"ì—´4\", \"\")\n",
    "            }\n",
    "\n",
    "            keywords = str(row_info[\"ì—´4\"]).split(',') if pd.notna(row_info[\"ì—´4\"]) else []\n",
    "            for kw in keywords[:max_keywords]:\n",
    "                kw = kw.strip()\n",
    "                if not kw:\n",
    "                    continue\n",
    "\n",
    "                print(f\"ğŸ” ({idx}) í‚¤ì›Œë“œ '{kw}' ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...\")\n",
    "                articles = get_article_urls_from_keyword(kw, articles_per_keyword)\n",
    "                if not articles:\n",
    "                    result = row_info.copy()\n",
    "                    result.update({\"í‚¤ì›Œë“œ\": kw, \"ì œëª©\": \"âŒ ë¬¸ì„œ ì—†ìŒ\", \"URL\": \"\", \"ì •ì˜\": \"N/A\"})\n",
    "                    all_results.append(result)\n",
    "                    continue\n",
    "\n",
    "                for title, url in articles:\n",
    "                    parsed = parse_article_contents(title, url)\n",
    "                    result = row_info.copy()\n",
    "                    result[\"í‚¤ì›Œë“œ\"] = kw\n",
    "                    result.update(parsed)\n",
    "                    all_results.append(result)\n",
    "                    time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        print(\"âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê¹Œì§€ ì €ì¥ í›„ ì¢…ë£Œí•©ë‹ˆë‹¤...\")\n",
    "\n",
    "    finally:\n",
    "        final_df = pd.DataFrame(all_results)\n",
    "        final_df.to_excel(save_path, index=False)\n",
    "        print(f\"âœ… ì €ì¥ ì™„ë£Œ: {save_path}\")\n",
    "\n",
    "# âœ… ì‹¤í–‰\n",
    "crawl_articles_from_excel_rows(\"split_4.xlsx\", max_keywords=2, articles_per_keyword=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12ff72d-9f8c-4b27-bef0-4ea0008a8865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
