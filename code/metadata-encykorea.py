import json
from collections import Counter
import numpy as np
import matplotlib.pyplot as plt


def fix_double_underscore(data):
    """metadata.id ë‚´ __ë¥¼ _ë¡œ ì¹˜í™˜"""
    def fix(item):
        if "metadata" in item and "id" in item["metadata"]:
            item["metadata"]["id"] = item["metadata"]["id"].replace("__", "_")
    if isinstance(data, list):
        for item in data:
            fix(item)
    else:
        fix(data)
    return data


def count_duplicate_contents(data):
    contents = [item.get("content", "").strip() for item in data]
    content_counts = Counter(contents)
    duplicate_contents = {k: v for k, v in content_counts.items() if v > 1}

    print(f"ğŸ“¦ ì „ì²´ í•­ëª© ìˆ˜: {len(contents)}")
    print(f"ğŸ“‘ ê³ ìœ  content ìˆ˜: {len(content_counts)}")
    print(f"ğŸ” ì¤‘ë³µëœ content ìˆ˜ (2ê°œ ì´ìƒ ë“±ì¥): {len(duplicate_contents)}")
    print(f"ğŸ§® ì¤‘ë³µ í•­ëª©ì˜ ì´ ê±´ìˆ˜: {sum(duplicate_contents.values())}")
    print(f"ğŸ“Š ì¤‘ë³µ ë¹„ìœ¨: {sum(duplicate_contents.values()) / len(contents) * 100:.2f}%")

    print("\nğŸ“Œ ê°€ì¥ ë§ì´ ì¤‘ë³µëœ content Top 10:")
    for content, count in content_counts.most_common(10):
        if count > 1:
            print(f"- {count}íšŒ: {content[:50]}{'...' if len(content) > 50 else ''}")


def merge_by_content(data):
    merged = {}
    for item in data:
        key = item['content']
        meta = item.get("metadata", {})

        if key not in merged:
            merged[key] = {
                "content": key,
                "url": item.get("url", ""),
                "metadata": {
                    "id": meta.get("id", ""),
                    "keyword": meta.get("keyword", ""),
                    "title": meta.get("title", ""),
                    "source": meta.get("source", ""),
                    "ë‚´ìš©": meta.get("ë‚´ìš©", {})
                }
            }
        else:
            existing_meta = merged[key]["metadata"]
            for field in ["id", "keyword"]:
                old_vals = existing_meta.get(field, "").split(",")
                new_vals = meta.get(field, "").split(",")
                merged_vals = set(v.strip() for v in old_vals + new_vals if v.strip())
                existing_meta[field] = ", ".join(sorted(merged_vals))

            if meta.get("title", "").strip() not in existing_meta["title"]:
                existing_meta["title"] += " | " + meta["title"].strip()

    return list(merged.values())


def remove_fields(data, fields_to_remove):
    for item in data:
        ë‚´ìš© = item.get("metadata", {}).get("ë‚´ìš©", {})
        if isinstance(ë‚´ìš©, dict):
            for key in fields_to_remove:
                ë‚´ìš©.pop(key, None)
    return data


def merge_ë‚´ìš©_into_content(data):
    for item in data:
        meta = item.get("metadata", {})
        original_content = item.get("content", "").strip()
        meta["title2"] = original_content
        ë‚´ìš© = meta.pop("ë‚´ìš©", None)

        if ë‚´ìš© and isinstance(ë‚´ìš©, dict):
            merged_text = "\n\n" + "\n".join(f"â–  {k}\n{v}" for k, v in ë‚´ìš©.items())
            item["content"] = original_content + merged_text
        else:
            item["content"] = original_content
    return data


def remove_url_fields(obj):
    """ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ ë‚´ë¶€ì˜ ëª¨ë“  url í‚¤ ì œê±° (ì¬ê·€)"""
    if isinstance(obj, dict):
        obj.pop("url", None)
        for value in obj.values():
            remove_url_fields(value)
    elif isinstance(obj, list):
        for item in obj:
            remove_url_fields(item)


def analyze_content_lengths(data):
    lengths = [len(item.get("content", "")) for item in data]
    print("ğŸ“Š content ê¸¸ì´ í†µê³„:")
    print(f"ì´ ë¬¸ì„œ ìˆ˜       : {len(lengths)}")
    print(f"ìµœì†Œ ê¸¸ì´        : {np.min(lengths)}")
    print(f"ìµœëŒ€ ê¸¸ì´        : {np.max(lengths)}")
    print(f"í‰ê·  ê¸¸ì´        : {np.mean(lengths):.2f}")
    print(f"ì¤‘ì•™ê°’ (median)  : {np.median(lengths)}")
    print(f"í‘œì¤€í¸ì°¨ (std)   : {np.std(lengths):.2f}")

    plt.hist(lengths, bins=50, color="skyblue", edgecolor="black")
    plt.title("Distribution of Content Lengths")
    plt.xlabel("Content Length (characters)")
    plt.ylabel("Number of Items")
    plt.grid(True)
    plt.show()

    # 80000ì ì´ìƒ í•„í„°ë§
    long_contents = [item for item in data if len(item.get("content", "")) > 80000]
    print("\nğŸ“Œ Length > 80000:")
    for item in long_contents:
        print(f"ID: {item.get('metadata', {}).get('title2', 'N/A')}")
    print(f"\nì´ {len(long_contents)}ê°œì˜ í•­ëª©ì´ 80000ì ì´ìƒì…ë‹ˆë‹¤.")


def transform_to_paragraph_units(data):
    transformed = []
    for i, item in enumerate(data, 1):
        meta = item.get("metadata", {})
        keyword = meta.get("keyword", "")
        main_title = item.get("content")
        ë‚´ìš© = meta.get("ë‚´ìš©", {})

        for j, (section_title, section_text) in enumerate(ë‚´ìš©.items(), 1):
            new_obj = {
                "content": f"{section_title}\n{section_text}\n",
                "metadata": {
                    "id": f"{i}-{j}",
                    "keyword": keyword,
                    "content_type": section_title,
                    "title": main_title
                }
            }
            transformed.append(new_obj)
    return transformed


def main():
    # 1. JSON ë¡œë“œ
    with open("ë¯¼ì¡±ëŒ€ë°±ê³¼_JSON_ìµœì¢…ë³¸.json", "r", encoding="utf-8") as f:
        data = json.load(f)

    data = fix_double_underscore(data)

    # 2. ì¤‘ë³µ content ë¶„ì„
    count_duplicate_contents(data)

    # 3. content ê¸°ì¤€ ë³‘í•©
    merged_data = merge_by_content(data)
    with open("merged_safe.json", "w", encoding="utf-8") as f:
        json.dump(merged_data, f, ensure_ascii=False, indent=2)

    # 4. í•„ìš” ì—†ëŠ” í•„ë“œ ì œê±°
    fields_to_remove = {"ëª©ì°¨", "ì§‘í•„ì", "ê´€ë ¨ ë¯¸ë””ì–´", "ì£¼ì„", "ì°¸ê³ ë¬¸í—Œ"}
    cleaned_data = remove_fields(merged_data, fields_to_remove)
    with open("cleaned_file.json", "w", encoding="utf-8") as f:
        json.dump(cleaned_data, f, ensure_ascii=False, indent=2)

    # 5. metadata["ë‚´ìš©"] â†’ contentë¡œ ë³‘í•©
    content_merged = merge_ë‚´ìš©_into_content(cleaned_data)
    with open("merged_content.json", "w", encoding="utf-8") as f:
        json.dump(content_merged, f, ensure_ascii=False, indent=2)

    # 6. url í•„ë“œ ì œê±°
    remove_url_fields(content_merged)
    with open("no_url_file.json", "w", encoding="utf-8") as f:
        json.dump(content_merged, f, ensure_ascii=False, indent=2)

    # 7. ë¶„ì„
    analyze_content_lengths(content_merged)

    # 8. ë‚´ìš© ë‹¨ìœ„ ë¶„í•´
    with open("cleaned_file.json", "r", encoding="utf-8") as f:
        original_data = json.load(f)
    transformed = transform_to_paragraph_units(original_data)
    with open("file.json", "w", encoding="utf-8") as f:
        json.dump(transformed, f, ensure_ascii=False, indent=2)


if __name__ == "__main__":
    main()
